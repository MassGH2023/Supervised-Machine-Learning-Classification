{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtViSB4j4t0/55xzm9Gkxx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassGH2023/Supervised-Machine-Learning-Classification/blob/main/Decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree\n",
        "\n",
        "A decision tree is a flowchart-like structure used to make decisions or predictions. It consists of nodes representing decisions or tests on attributes, branches representing the outcome of these decisions, and leaf nodes representing final outcomes or predictions. Each internal node corresponds to a test on an attribute, each branch corresponds to the result of the test, and each leaf node corresponds to a class label or a continuous value.\n",
        "\n",
        "\n",
        "**How Decision Trees Work?**\n",
        "\n",
        "The process of creating a decision tree involves:\n",
        "\n",
        "* Selecting the Best Attribute: Using a metric like Gini impurity, entropy, or information gain, the best attribute to split the data is selected.\n",
        "* Splitting the Dataset: The dataset is split into subsets based on the selected attribute.\n",
        "* Repeating the Process: The process is repeated recursively for each subset, creating a new internal node or leaf node until a stopping criterion is met (e.g., all instances in a node belong to the same class or a predefined depth is reached)."
      ],
      "metadata": {
        "id": "JV0Ndl34NUCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm selects the feature that best splits the data at each decision node, often using metrics like Gini impurity, Entropy (for classification), or Mean Squared Error (for regression).\n",
        "\n",
        "**Determining When to Stop Splitting:**\n",
        "\n",
        "Maximum Depth: The tree stops growing after reaching a specified maximum number of splits (depth). For example, if the maximum depth is 3, the tree can have up to 3 levels of splits.\n",
        "\n",
        "Minimum Samples per Leaf: The tree stops if a split results in a leaf node with fewer than a specified number of samples. For example, if you set this to 2, the tree wonâ€™t split if the resulting leaf node has only 1 sample.\n",
        "\n",
        "Maximum Number of Nodes: The tree stops if it reaches a predefined number of nodes (splits).\n",
        "\n",
        "Pure Nodes: If all data points in a node belong to the same class (or are very similar), the tree stops splitting. This is a \"pure\" node where no further splits are needed.\n",
        "\n",
        "No Improvement: If splitting a node does not significantly improve the classification or regression performance (based on a metric like Gini or MSE), the tree stops."
      ],
      "metadata": {
        "id": "s2Kzh4fnSbd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics for Splitting**\n",
        "\n",
        "1. Gini Impurity\n",
        "2. Entropy\n",
        "3. Information Gain"
      ],
      "metadata": {
        "id": "AnhXLZboXcQG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogG-TvGANIyy"
      },
      "outputs": [],
      "source": []
    }
  ]
}