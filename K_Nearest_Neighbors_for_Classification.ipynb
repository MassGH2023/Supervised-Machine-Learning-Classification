{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlOMhIVdBG7Ayv+ObNvjGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassGH2023/Supervised-Machine-Learning-Classification/blob/main/K_Nearest_Neighbors_for_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K Nearest Neighbors (KNN) algorithm has several key **assumptions**:\n",
        "\n",
        "Feature Relevance: It assumes that the features used for classification or regression are relevant and contribute meaningfully to the outcome.\n",
        "\n",
        "Distance Metric: KNN relies on a distance metric (like Euclidean distance) to determine the \"closeness\" of data points. It assumes that the chosen distance metric is appropriate for the data.\n",
        "\n",
        "Locality: KNN assumes that similar instances are close to each other in the feature space. This means that the class of a data point can be inferred from its neighbors.\n",
        "\n",
        "Feature Scaling: KNN assumes that the features are on a similar scale. If not, features with larger ranges can disproportionately influence the distance calculations. Therefore, feature scaling (normalization or standardization) is often necessary.\n",
        "\n",
        "Independence of Features: While KNN can handle correlated features, it generally assumes that the features are independent of each other."
      ],
      "metadata": {
        "id": "YHW-tu6Z47HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using the K Nearest Neighbors (KNN) algorithm, there are several **key considerations** to keep in mind:\n",
        "\n",
        "Choice of K:\n",
        "\n",
        "The value of K (number of neighbors) significantly affects the model's performance. A small K can lead to overfitting, while a large K can smooth out the decision boundary too much, leading to underfitting.\n",
        "        It's often beneficial to experiment with different values of K and use cross-validation to find the optimal one.\n",
        "\n",
        "Distance Metric:\n",
        "\n",
        "The choice of distance metric (e.g., Euclidean, Manhattan, Minkowski) can impact the results. Different metrics may yield different neighbors, so it's important to choose one that fits the data characteristics.\n",
        "\n",
        "Feature Scaling:\n",
        "\n",
        "Since KNN relies on distance calculations, it's crucial to scale features to ensure that they contribute equally. Techniques like normalization or standardization are commonly used.\n",
        "\n",
        "Dimensionality:\n",
        "\n",
        "High-dimensional data can lead to the \"curse of dimensionality,\" where the distance between points becomes less meaningful. Dimensionality reduction techniques (like PCA) may be necessary to improve performance.\n",
        "\n",
        "Handling Imbalanced Data:\n",
        "\n",
        "If the classes are imbalanced, KNN may be biased towards the majority class. Techniques like oversampling, undersampling, or using weighted KNN can help address this issue.\n",
        "\n",
        "Computational Efficiency:\n",
        "\n",
        "KNN can be computationally expensive, especially with large datasets, as it requires calculating distances to all training samples for each prediction. Consider using data structures like KD-trees or Ball trees to speed up the process.\n",
        "\n",
        "Noise Sensitivity:\n",
        "\n",
        "KNN is sensitive to noise in the data. Outliers can significantly affect the predictions, so it's important to preprocess the data to handle noise effectively."
      ],
      "metadata": {
        "id": "qCc-PiLV5Y2Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkC10Eyc42Fn"
      },
      "outputs": [],
      "source": []
    }
  ]
}