{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MassGH2023/Supervised-Machine-Learning-Classification/blob/main/lab_jupyter_logistic_regression_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f804a730-4bc9-4b94-afe2-f67a7ea73fa7"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0cbcb96-5249-4638-83a3-2412908c73ea"
      },
      "source": [
        "# **Logistic Regression**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6558fab-d72c-47f8-bf43-68272cb19582"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a30f68-b715-44ef-85d8-083d50a9a650"
      },
      "source": [
        "In this lab, you will learn about and get hands-on practice with the logistic regression model, a popular and effective classification model. Understanding logistic regression and being able to apply it to classification tasks is essential because logistic regression models form the fundamentals of neural networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bf65a9-a287-46e6-acbc-4792e1804d16"
      },
      "source": [
        "We will use a real-world dataset that contains detailed nutrition information about food items for people with diabetes. The objective is to classify whether a diabetic patient should choose More Often, Less Often, or In Moderation for a specific food item based on the nutrition information in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40cbf617-2cc0-4ef0-9849-989046d731b0"
      },
      "source": [
        "## Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73f78153-9c7e-44d5-9f09-382f0bdeef39"
      },
      "source": [
        "After completing this lab you will be able to:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8914e4b-6c98-43a6-8c67-67956b3f7a54"
      },
      "source": [
        "*   Preprocess and generate training and testing datasets\n",
        "*   Train and fine-tune logistic regression models\n",
        "*   Interpret trained logistic regression models\n",
        "*   Evaluate trained logistic regression models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33349d31-abac-46a4-b8b6-91ea5817a251"
      },
      "source": [
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8357b7b4-b516-4085-877e-fa14eb70ff2a"
      },
      "source": [
        "## Prepare and setup lab environment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08cff741-f38c-4221-88d8-0072367a289f"
      },
      "outputs": [],
      "source": [
        "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
        "# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n",
        "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec180963-1d3b-4ebf-8fb5-92e6581ab7cf"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install seaborn\n",
        "!pip install matplotlib"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75f3d6ed-9307-467a-83c1-84997e3c26ed"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, learning_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,ConfusionMatrixDisplay, precision_recall_fscore_support, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34dc8dcb-2474-4c83-9533-8a7bc64638ee"
      },
      "outputs": [],
      "source": [
        "# also set a random state\n",
        "rs = 123"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a907f574-bc36-44e0-a761-70bc192328f6"
      },
      "source": [
        "### Exploratory Data Analysis(EDA) and Feature Engineering\n",
        "Before we get to the model implementation, it is essential to examine the dataset and carefully select the features that will serve as inputs for the model..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd804009-bbb1-492e-aa0b-3e0c5dab24b2"
      },
      "source": [
        "### Load and explore the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91382f14-8a24-47ae-8c60-bc782201b7db"
      },
      "source": [
        "First, let's load the dataset as a `Pandas` dataframe and conduct some basic EDA tasks on it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ecd8ca8-bf25-4001-8c9c-1174798f7ab8"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/food_items.csv\"\n",
        "food_df = pd.read_csv(dataset_url)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6b2e35-f6e5-4a3b-aceb-20cf67082ae8"
      },
      "source": [
        "And, let's quickly check its column types.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3e1d137-e748-4df7-b38f-630a5d9d08c8"
      },
      "outputs": [],
      "source": [
        "food_df.dtypes"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2908718-bba6-43ec-aa74-f9fe1c9797e2"
      },
      "source": [
        "Print the first ten food items:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f449a552-2200-423d-9375-b2cb3de5a5b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "food_df.head(10)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0707351b-93e8-400f-84cb-df3ba072b9d5"
      },
      "source": [
        "Get the row entries with col 0 to -1 (16).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "068549da-9acc-4b90-a810-aebd7b715703"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_cols = list(food_df.iloc[:, :-1].columns)\n",
        "feature_cols"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6a2b19f-df93-44da-9e25-4cc1f4110069"
      },
      "source": [
        "Obtain descriptive statistics:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607824c4-3061-454d-b43e-a3550049dd90"
      },
      "outputs": [],
      "source": [
        "food_df.iloc[:, :-1].describe()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "427af3fd-94b7-4b73-9696-a043dfe6eda4"
      },
      "source": [
        "As we can see from the above output, this dataset contains 17 nutrient categories about each food item. These categories include Calories, Total Fat, Protein, Sugar, etc., and are listed as numeric variables. As such, we only need to scale them for training our logistic regression model so that we can compare our feature coefficients directly. This will be done under the feature engineering section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5988d8-3f41-4c8c-9eab-3348c31ec847"
      },
      "source": [
        "Next, let's check the target variable in the `class` column to see the label values and their distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cda24881-4747-4d20-a168-c08969e6eda6"
      },
      "outputs": [],
      "source": [
        "# # Get the row entries with the last col 'class'\n",
        "food_df.iloc[:, -1:].value_counts(normalize=True)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50da1eb7-adde-4bc8-b279-fd87b4242a8d"
      },
      "outputs": [],
      "source": [
        "food_df.iloc[:, -1:].value_counts().plot.bar(color=['yellow', 'red', 'green'])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caba84ff-90e0-4740-b759-61d78aec2724"
      },
      "source": [
        "As we can see from the bar chart above, this dataset has three classes: `In Moderation`, `Less Often`, and `More Often`. The three labels are imbalanced. For diabetic patients, most food items are in the In Moderation and Less Often categories. This makes diabetes diet management very hard, so we could build a machine learning model to help patients choose their food.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "314d60c7-9152-4054-acfd-57fa132034b1"
      },
      "source": [
        "We have three labels meaning our logistic regression model will be multinomial with three classes.\n",
        "\n",
        "A multinomial logistic regression is a generalized logistic regression model which generates a probability distribution over all classes, based on the logits or exponentiated log-odds calculated for each class (usually more than two).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbdeaf25-86bc-4368-b73f-193e0b2cb0d2"
      },
      "source": [
        "Also note that a multinomial logistic regression model is different from the `one-vs-rest` binary logistic regression. For `one-vs-rest` schema, you need to train an independent classifier for each class. For example, you need a `More Often` classifier to differentiate a food item between `More Often` and `Not More Often` (or, `In Moderation` and `Less Often`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff23c1a4-1d26-4a54-99bb-b6a446c5d17f"
      },
      "source": [
        "### Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af27fff3-1b59-41b6-8d8a-98078cbf5a5f"
      },
      "source": [
        "Now you should have some basic understanding about the food dataset. Next, let's process the raw dataset and construct input data `X` and label/output `y` for logistic regression model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e92f7ffb-e580-4268-a608-263148c40f3f"
      },
      "outputs": [],
      "source": [
        "X_raw = food_df.iloc[:, :-1]\n",
        "y_raw = food_df.iloc[:, -1:]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6028199-895b-4e8b-9131-f24b109ea8ef"
      },
      "source": [
        "Fortunately, all feature columns are numeric so we just need to scale them. Here we use the `MinMaxScaler` provided by `sklearn` for scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c9f15c1-8f9d-4157-ad5b-4ae330135835"
      },
      "outputs": [],
      "source": [
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a70fdd68-aae1-427a-830e-22d24cab9cce"
      },
      "outputs": [],
      "source": [
        "# Scaling the raw input features\n",
        "X = scaler.fit_transform(X_raw)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e31a983-9e29-40d6-9644-7ce36b03d12a"
      },
      "source": [
        "Let's check the scaled feature value range:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e9e0131-97ba-476d-8275-ca7b1ea749a8"
      },
      "outputs": [],
      "source": [
        "print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dfe4c52-fdd2-4dbd-8e47-1020078684d4"
      },
      "source": [
        "For the target variable `y`, let's use the `LabelEncoder` provided by `sklearn` to encode its three class values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efbfddd2-5b64-407c-b685-c8ffeabd6cbd"
      },
      "outputs": [],
      "source": [
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6bacfd1-7fa2-47bf-8b3d-011209311eb2"
      },
      "outputs": [],
      "source": [
        "# Encode the target variable\n",
        "y = label_encoder.fit_transform(y_raw.values.ravel())\n",
        "# Note that ravel() function flattens the vector."
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef13d0a9-a8e7-455d-b691-02a9cc0c88bd"
      },
      "source": [
        "The encoded target variable will only contain values `0=In Moderation`, `1=Less Often`, `2=More Often`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbc0ed6c-80b8-4ecb-913c-77e1959fd249"
      },
      "outputs": [],
      "source": [
        "np.unique(y, return_counts=True)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e484528-db33-4f3f-9f1d-79b5a8172ead"
      },
      "source": [
        "## Train logistic regression models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f5376b-3349-4371-9555-acdf21f4f198"
      },
      "source": [
        "First, let's split the dataset into a training and a testing dataset. Training dataset will be used to train and (maybe) tune models, and testing dataset will be used to evaluate the models. Note that you may also split the training dataset into train and validation sets where the validation dataset is only used to tune the model and to set the model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b80e6758-8396-4681-b25e-c03b76e1032e"
      },
      "outputs": [],
      "source": [
        "# First, let's split the training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41679ec3-cafb-424f-ad24-0be82e383d30"
      },
      "source": [
        "Let's look at the shapes of the split datasets:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "192e093d-c561-4bc4-b60d-09bb5ad439f0"
      },
      "outputs": [],
      "source": [
        "print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f6873f5-bfce-490b-8390-62b51812c6e4"
      },
      "outputs": [],
      "source": [
        "print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4693b8a0-db8c-4307-bcce-c9be2778e02a"
      },
      "source": [
        "OK, now we have the training and testing datasets ready, let's start the model training task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d747019-0f7f-489d-a7b6-1d9598c7c9b6"
      },
      "source": [
        "We first define a `sklearn.linear_model.LogisticRegression` model with the following arguments, you can check the comment for each argument for what it means.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "115f3773-ff80-4553-9e3e-1fd9135a524b"
      },
      "outputs": [],
      "source": [
        "# L2 penalty to shrink coefficients without removing any features from the model\n",
        "penalty= 'l2'\n",
        "# Our classification problem is multinomial\n",
        "multi_class = 'multinomial'\n",
        "# Use lbfgs for L2 penalty and multinomial classes\n",
        "solver = 'lbfgs'\n",
        "# Max iteration = 1000\n",
        "max_iter = 1000"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97274b82-6d99-4eb4-88a3-c1e4318c621e"
      },
      "outputs": [],
      "source": [
        "# Define a logistic regression model with above arguments\n",
        "l2_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter=max_iter)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474e5114-e4d4-4f6d-91c1-ea15eb3779d0"
      },
      "source": [
        "Let's train the model with training input data `X_train` and labels `y_train`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "673035b3-97d3-4c77-992a-3d54cd8a238e"
      },
      "outputs": [],
      "source": [
        "l2_model.fit(X_train, y_train)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bd536d2-0dba-414c-a241-00da5a3cdfce"
      },
      "outputs": [],
      "source": [
        "l2_preds = l2_model.predict(X_test)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea361f7-8449-4ad5-93c3-8ee070563b0a"
      },
      "source": [
        "Because we may need to evaluate the model multiple times with different model hyper parameters, here we define an utility method to take the ground truths `y_test` and the predictions `preds`, and return a Python `dict` with `accuracy`, `recall`, `precision`, and `f1score`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3770eb92-a2fc-42cf-9da7-b44f18f0af1e"
      },
      "outputs": [],
      "source": [
        "def evaluate_metrics(yt, yp):\n",
        "    results_pos = {}\n",
        "    results_pos['accuracy'] = accuracy_score(yt, yp)\n",
        "    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp)\n",
        "    results_pos['recall'] = recall\n",
        "    results_pos['precision'] = precision\n",
        "    results_pos['f1score'] = f_beta\n",
        "    return results_pos"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8328bb8-76e8-4fce-af39-4a07c4120c07"
      },
      "outputs": [],
      "source": [
        "evaluate_metrics(y_test, l2_preds)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eca5efc8-64fa-4def-a16e-f950ba62f417"
      },
      "source": [
        "As we can see from  the above evaluation results, the logistic regression model has relatively good performance on this multinomial classification task. The overall accuracy is around `0.77` and the f1score is around `0.8`. Note that for `recall`, `precision`, and `f1score`, we output the values for each class to see how the model performs on an individual class. And, we can see from the results, the recall for `class=2` (More often) is not very good. This is actually a common problem called imbalanced classification challenge. We will introduce solution to this problem later in this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30a1175e-e296-4e10-bef5-41fa1393954c"
      },
      "source": [
        "Next, let's try defining another logistic regression model with l1 penality this time, to see if our classification performance would be improved.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c02cfc8f-8381-4237-add7-fe7dfe8a45ef"
      },
      "outputs": [],
      "source": [
        "# L1 penalty to shrink coefficients without removing any features from the model\n",
        "penalty= 'l1'\n",
        "# Our classification problem is multinomial\n",
        "multi_class = 'multinomial'\n",
        "# Use saga for L1 penalty and multinomial classes\n",
        "solver = 'saga'\n",
        "# Max iteration = 1000\n",
        "max_iter = 1000"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a4dfc78-5fd5-439f-a01c-107d5ca4b1b0"
      },
      "source": [
        "Then we define another logistic regression model with above arguments using l1 penality and related solver.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c00344cf-66f2-4f11-87af-6d8434b91b3d"
      },
      "outputs": [],
      "source": [
        "# Define a logistic regression model with above arguments\n",
        "l1_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter = 1000)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6a1c017-51bf-4587-bd95-3a28167ce932"
      },
      "source": [
        "We can start to train the new `l1_model` with the new taining dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa4a6c7a-9278-4c7b-8fd7-52b3eacc3f69"
      },
      "outputs": [],
      "source": [
        "l1_model.fit(X_train, y_train)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46b487a9-03b5-4275-8641-8ef56baf0fa0"
      },
      "source": [
        "And, make predictions using the input in the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84aa7c27-3b23-4616-8790-f88e6b8bc0c2"
      },
      "outputs": [],
      "source": [
        "l1_preds = l1_model.predict(X_test)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d78729f-d448-44cb-baf3-b4e5c8e86f8a"
      },
      "source": [
        "We can also check the class probability distribution using the `predict_proba` function. For example, we want to see the probabilities of belonging to each class for the first instance in the test dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe968108-8b20-42a9-ad60-471cbf3e02e5"
      },
      "outputs": [],
      "source": [
        "odd_ratios = l1_model.predict_proba(X_test[:1, :])[0]\n",
        "odd_ratios"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8d9b37-d831-4680-a46a-852324c63a28"
      },
      "source": [
        "We can see that  Class 1 has the largest probability 0.96. As such, the model prediction for this instance will be class `1` and this is the same as the `predict` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "637db0d3-2abb-4df1-835f-4db9599bf196"
      },
      "outputs": [],
      "source": [
        "l1_model.predict(X_test[:1, :])[0]"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1065ae07-fcf8-4b34-a842-bf41a8240ce2"
      },
      "source": [
        "Given the true labels (`y_test`) and predictions, we can evaluate the model performance by calling the utility `evaluate_metrics`  method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "242b6412-b3a7-441f-9e64-3b48b391bea5"
      },
      "outputs": [],
      "source": [
        "evaluate_metrics(y_test, l1_preds)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eac1b672-3390-4447-a859-1b8f77ba2715"
      },
      "source": [
        "Now, we can see this logistic regression with l1 penalty has much better performance than l2. One possible reason is that l1 penalty may remove some correlated feature variables by shrinking their coefficents to zero. As such, the model is much simplified to avoid overfitting on the training data and better aligned with the logistic regression assumption that all features should be independent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c18eb4c1-5fe9-4fea-93fd-7ec101c93bdd"
      },
      "source": [
        "### Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa7e8847-e9b1-41aa-9b2a-f053b173c283"
      },
      "source": [
        "We can also plot the confusion matrix based on the true labels and predictions using the `confusion_matrix` method provided by `sklearn`,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fd8f0a9-dd88-4e4d-a8a2-c9864d2d9f41"
      },
      "outputs": [],
      "source": [
        "cf = confusion_matrix(y_test, l1_preds, normalize='true')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9019f89b-4991-4e9b-8abe-9785e234bee8"
      },
      "source": [
        "and easily visualize it using a heatmap method provided by `seaborn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861d6fa6-7890-46bb-b561-59650d78bee1"
      },
      "outputs": [],
      "source": [
        "sns.set_context('talk')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=l1_model.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfc753e7-5ce4-4c0c-8a1c-41ba92fad8cb"
      },
      "source": [
        "### Interpret logistic regression models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb0ea663-1f6c-4a25-a146-d25d41f27a43"
      },
      "source": [
        "One way to interpret logistic regression models is by analyzing feature coefficients. Although it may not be as effective as the regular linear regression models because the logistic regression model has a sigmoid function, we can still get a sense for the importance or impact of each feature.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c451423-92de-4753-a3bd-adbcfae24976"
      },
      "source": [
        "We can check the coefficients for logistic regression model using its `coef_` attribute:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6fe8264-22e9-4ea9-87c4-f32945eda53e"
      },
      "outputs": [],
      "source": [
        "l1_model.coef_"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67c4a57-ad77-43d7-b335-20e92df86072"
      },
      "source": [
        "The `coef_` is a coefficients list with three elements, one element is the actual coefficent for class 0, 1, 2. To better analyze the coefficients, let's use three utility methods to sort and visualize them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "439f4fcc-7f6c-4e28-94a9-35f570cec8f9"
      },
      "outputs": [],
      "source": [
        "# Extract and sort feature coefficients\n",
        "def get_feature_coefs(regression_model, label_index, columns):\n",
        "    coef_dict = {}\n",
        "    for coef, feat in zip(regression_model.coef_[label_index, :], columns):\n",
        "        if abs(coef) >= 0.01:\n",
        "            coef_dict[feat] = coef\n",
        "    # Sort coefficients\n",
        "    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n",
        "    return coef_dict\n",
        "\n",
        "# Generate bar colors based on if value is negative or positive\n",
        "def get_bar_colors(values):\n",
        "    color_vals = []\n",
        "    for val in values:\n",
        "        if val <= 0:\n",
        "            color_vals.append('r')\n",
        "        else:\n",
        "            color_vals.append('g')\n",
        "    return color_vals\n",
        "\n",
        "# Visualize coefficients\n",
        "def visualize_coefs(coef_dict):\n",
        "    features = list(coef_dict.keys())\n",
        "    values = list(coef_dict.values())\n",
        "    y_pos = np.arange(len(features))\n",
        "    color_vals = get_bar_colors(values)\n",
        "    plt.rcdefaults()\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.barh(y_pos, values, align='center', color=color_vals)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(features)\n",
        "    # labels read top-to-bottom\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel('Feature Coefficients')\n",
        "    ax.set_title('')\n",
        "    plt.show()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "446f04b4-6157-47df-9fc4-7d52a4c1b903"
      },
      "source": [
        "Then, let's visualize the sorted coefficient for class 1, the `Less Often` class:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64657e44-12e3-4af4-9996-58b88cfc4491"
      },
      "outputs": [],
      "source": [
        "# Get the coefficents for Class 1, Less Often\n",
        "coef_dict = get_feature_coefs(l1_model, 1, feature_cols)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2d5397f-5585-492e-b8ea-4e5b4ff86582"
      },
      "outputs": [],
      "source": [
        "visualize_coefs(coef_dict)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893cea90-80cd-4720-afa9-180e58e31145"
      },
      "source": [
        "As we can see, unhealthy nutrients such as Saturated Fat, Sugars, Cholesterol, Total Fat, etc., have high positive coefficients. Food items containing unhealthy nutrients will have higher coeficients and will be more likely to be categorized in the 'Less Often' class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6e27a8-d49e-49d1-924e-33632f6ea5b2"
      },
      "source": [
        "Next, let's see the coefficents for Class 2, `More Often`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08608e5e-ac32-4d2f-bbfd-845428a75af1"
      },
      "outputs": [],
      "source": [
        "# Coefficients for Class 2\n",
        "coef_dict = get_feature_coefs(l1_model, 2, feature_cols)\n",
        "visualize_coefs(coef_dict)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57d5911d-75aa-4d0b-a76e-2d2f0d0c7a8e"
      },
      "source": [
        "Conversely, if a food item has a high amount of calories, total carbohydrates, and total fat, then it is unlikely to be categorized in the 'More Often' class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b67d4391-8109-4fbb-942e-b771d0e88e05"
      },
      "source": [
        "## Coding Exercise: Train and evaluate a logistic regression model with elastic-net penality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82b9369c-773d-4d97-8a49-bab9fa81cdbf"
      },
      "source": [
        "Now, it's your turn to walk through the end-to-end process of defining, building, evaluating, and interpreting a logistic regression model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dd533d6-f84f-4e5f-b626-58a2fe12d4b7"
      },
      "source": [
        "### Define a logistic regression with elastic-net penality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66b412bc-a175-4e8b-95ac-9ed5d934e205"
      },
      "outputs": [],
      "source": [
        "# Type your code here\n",
        "# HINT: sklearn only support saga solver for elastic-net penality\n",
        "# and you need to set another l1_ratio to be within 0 < l1_ratio <1, in order to actually use elastic-net\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d866092a-3381-4431-9200-96e2df3649c9"
      },
      "source": [
        "### Train the model with training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e13383d1-cb6f-4f4a-a960-38058b7c06ea"
      },
      "outputs": [],
      "source": [
        "# Type your code here\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e023c8a0-67a6-4f65-b4c5-de2250cfa4da"
      },
      "source": [
        "### Evaluate the model using accuracy, precision, recall, and F1score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af159b6f-c821-4e0c-a4f9-e0be2534af44"
      },
      "outputs": [],
      "source": [
        "# Type your code here\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2582ca5-6140-43bf-8a48-d69c66d88618"
      },
      "source": [
        "### Plot confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932cb08f-9459-41b9-8242-1ed65c58654f"
      },
      "outputs": [],
      "source": [
        "# Type your code here\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dadb3b5-4013-48c5-91d5-c3ef1b29a86b"
      },
      "source": [
        "### Interpret the model by analysing its coefficients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc51d333-32ec-486a-9ca2-abfccb879a37"
      },
      "outputs": [],
      "source": [
        "# Type your code here\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a096d569-750e-4a67-b186-96b805d1721b"
      },
      "source": [
        "<details><summary>Click here for a sample solution</summary>\n",
        "\n",
        "```python\n",
        "# elasticnet penalty to shrink coefficients without removing any features from the model\n",
        "penalty= 'elasticnet'\n",
        "# Our classification problem is multinomial\n",
        "multi_class = 'multinomial'\n",
        "# Use saga for L1 penalty and multinomial classes\n",
        "solver = 'saga'\n",
        "# Max iteration = 1000\n",
        "max_iter = 1000\n",
        "# l1_ratio\n",
        "l1_ratio = 0.1\n",
        "\n",
        "# Define a elastic-net model\n",
        "en_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter = 1000, l1_ratio=l1_ratio)\n",
        "en_model.fit(X_train, y_train)\n",
        "# Make predictions\n",
        "preds = en_model.predict(X_test)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64dbeb7a-02fd-482c-9341-3aac224062a6"
      },
      "source": [
        "## Next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eda3f1c3-51a1-473c-bf61-1fcaa9176ee0"
      },
      "source": [
        "Great! Now you have learned about and practiced applying a logistic regression model to solve a real-world food classification problem for diabetic patients. You also learned how to evaluate and interpret the trained logistic regression models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dceb131e-4ee6-4907-b281-2c9268eac04c"
      },
      "source": [
        "Next, you will be learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68e2d833-8d2f-4014-b138-c2b67f753a25"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16b3105b-9845-4fb6-8760-c80ba2c017c2"
      },
      "source": [
        "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2021-01-01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5785d94-1629-42c4-a2e5-73621717fa4a"
      },
      "source": [
        "### Other Contributors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ec9d18-dc5b-45c8-95cb-e85f076b5941"
      },
      "source": [
        "<!--## Change Log--!>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18894628-f910-464d-9cc6-375f9f4038cc"
      },
      "source": [
        "<!--| Date (YYYY-MM-DD) | Version | Changed By | Change Description          |\n",
        "| ----------------- | ------- | ---------- | --------------------------- |\n",
        "| 2021-10-25        | 1.0     | Yan        | Created the initial version |\n",
        "--!>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17dcde8-d862-469f-bdac-f4a5e9ff08e0"
      },
      "source": [
        "Copyright © 2021 IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "650750a7c481003d0f489aa2ab268ed3c6e79f8c71e9359d815bdee7f137b02f",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}